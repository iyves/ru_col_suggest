{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/iyves/ru_col_suggest/blob/master/train_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06et0bNbcPjN"
   },
   "source": [
    "This colab notebook extrapolates the process of learning static and dynamic word embedding models for the task of erroneous collocation correction. The models are trained on text from the CAT and cybercat corpora, with the text for each respective corpora in a .txt file. The file should consist of sentences delimited with newline. \n",
    "\n",
    "Training data can be in raw tokens or lemmatized tokens, and may contain PoS tags (i.e. lemma_N). Additionally, lemmatization can be done using MyStem, UDPipe, or TreeTagger.\n",
    "\n",
    "**Note:** Run this colab with GPUs and High RAM. During training, do not leave this page and every hour or so click on a cell to ensure that the session remains active and doesn't prematurely disconnect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGYkWjKudOHP"
   },
   "outputs": [],
   "source": [
    "# This code assumes that the training data is stored in gdrive at `./drive/MyDrive/Training_Data/`\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wa92-k8hd18p"
   },
   "source": [
    "## Training of static word embeddings (w2v, fastText, GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KANhYg0dd-A"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from gensim.models import FastText, Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "import tempfile\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Load the training data\n",
    "paths = [  \n",
    "  # Lemmatized w/ treetagger\n",
    "  str(Path('./drive/MyDrive/Training_Data/CAT_sentences_full_treetagger_lemma_1.txt')),\n",
    "  str(Path('./drive/MyDrive/Training_Data/CAT_sentences_full_treetagger_lemma_2.txt')),\n",
    "  str(Path('./drive/MyDrive/Training_Data/cybercat_sentences_full_treetagger_lemma.txt'))\n",
    "\n",
    "  # Lemmatized w/ UDPipe\n",
    "  #  str(Path('./drive/MyDrive/Training_Data/CAT_sentences_full_lemma_1.txt')),\n",
    "  #  str(Path('./drive/MyDrive/Training_Data/CAT_sentences_full_lemma_2.txt')),\n",
    "  #  str(Path('./drive/MyDrive/Training_Data/cybercat_sentences_full_lemma.txt'))\n",
    "]\n",
    "\n",
    "class Corpus:\n",
    "  def __iter__(self):\n",
    "    for data_path in paths:\n",
    "      for line in open(data_path, \"r\", encoding=\"utf-8\"):\n",
    "          yield utils.simple_preprocess(line)\n",
    "\n",
    "sentences = Corpus()\n",
    "\n",
    "# For evaluating the quality of the word2vec model during training after each epoch\n",
    "# https://datascience.stackexchange.com/questions/9819/number-of-epochs-in-gensim-word2vec-implementation\n",
    "class LossLogger(CallbackAny2Vec):\n",
    "    '''Output loss at each epoch'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f'Epoch: {self.epoch}', end='\\t')\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        self.losses.append(loss)\n",
    "        print(f'  Loss: {loss}')\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNV81ZRLeMbs"
   },
   "outputs": [],
   "source": [
    "# Train w2v embeddings\n",
    "# Note: make sure that the path exists beforehand: `./drive/MyDrive/models/lemma/w2v/`\n",
    "CONTEXT_WINDOW = 5 # 5, 10\n",
    "MIN_COUNT = 5\n",
    "EPOCHS = 30\n",
    "SIZE = 500 # 200, 300, 500\n",
    "\n",
    "w2v_loss_logger = LossLogger()\n",
    "w2v_model = Word2Vec(sentences=sentences, size=SIZE, window=CONTEXT_WINDOW, min_count=MIN_COUNT, workers=10, iter=EPOCHS, \n",
    "                     callbacks=[w2v_loss_logger], compute_loss=True,)\n",
    "w2v_model.save('./drive/MyDrive/models/lemma/w2v/w2v_treetagger.model')\n",
    "print(w2v_loss_logger.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9iJlYabeYkH"
   },
   "outputs": [],
   "source": [
    "# Train fastText embeddings\n",
    "# Note: make sure that the path exists beforehand: `./drive/MyDrive/models/lemma/fastText/`\n",
    "CONTEXT_WINDOW = 5 # 5, 10\n",
    "MIN_COUNT = 5\n",
    "EPOCHS = 30\n",
    "SIZE = 500 # 200, 300, 500\n",
    "\n",
    "#note: the LossLogger doesn't work for fastText at this moment in time\n",
    "fastText_model = FastText(sentences=sentences, size=SIZE, window=CONTEXT_WINDOW, min_count=MIN_COUNT, workers=10, iter=EPOCHS)\n",
    "fastText_model.save('./drive/MyDrive/models/lemma/fastText/fastText_treetagger.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycRXD3laeuZv"
   },
   "outputs": [],
   "source": [
    "# Train GloVe embeddings\n",
    "# https://stackoverflow.com/questions/48962171/how-to-train-glove-algorithm-on-my-own-corpus\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "CONTEXT_WINDOW = 5 # 5, 10\n",
    "# MIN_COUNT = 5 note: in future, figure out how to set min_count for GloVe via this library\n",
    "EPOCHS = 30\n",
    "SIZE = 500 # 200, 300, 500\n",
    "\n",
    "#Training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentences, window=CONTEXT_WINDOW)\n",
    "\n",
    "glove = Glove(no_components=SIZE, learning_rate=0.05) \n",
    "glove.fit(corpus.matrix, epochs=EPOCHS, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('./drive/MyDrive/models/lemma/glove/glove_treetagger.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kE04uC0miySE"
   },
   "outputs": [],
   "source": [
    "# Convert GloVe embeddings into w2v format for use with the Gensim library\n",
    "# https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/2020/08/15/Intro_NLP_WordEmbeddings_Classification.html#Word-Embeddings,-GloVe-and-Text-classification\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_filename = \"./drive/MyDrive/models/lemma/glove/glove_treetagger.model\"\n",
    "word2vec_output_file = glove_filename+'.word2vec'\n",
    "glove2word2vec(glove_filename, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO6QMpPMkjPt"
   },
   "source": [
    "## Training of dynamic word embeddings (RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qE5Q-T3bkp-d"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y tensorflow\n",
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip list | grep -E 'transformers|tokenizers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTp9UCVelmKB"
   },
   "outputs": [],
   "source": [
    "# Train a tokenizer on the corpus \n",
    "# This step is necessary for training from scratch or if the pretrained model doesn't have a tokenizer\n",
    "\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "\n",
    "paths = [  \n",
    "  # Lemmatized w/ treetagger\n",
    "  str(Path('./drive/MyDrive/Training_Data/CAT_sentences_full_treetagger_lemma_1.txt')),\n",
    "  str(Path('./drive/MyDrive/Training_Data/CAT_sentences_full_treetagger_lemma_2.txt')),\n",
    "  str(Path('./drive/MyDrive/Training_Data/cybercat_sentences_full_treetagger_lemma.txt'))\n",
    "\n",
    "  # Lemmatized w/ UDPipe\n",
    "  #  str(Path('./drive/MyDrive/Training_Data/CAT_sentences_full_lemma_1.txt')),\n",
    "  #  str(Path('./drive/MyDrive/Training_Data/CAT_sentences_full_lemma_2.txt')),\n",
    "  #  str(Path('./drive/MyDrive/Training_Data/cybercat_sentences_full_lemma.txt'))\n",
    "]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "# ./drive/MyDrive/models/lemma/RuBERT_treetagger_lemma\n",
    "# ./drive/MyDrive/models/lemma/RuBERT_udpipe_lemma\n",
    "\n",
    "tokenizer.save_model(\"./drive/MyDrive/models/lemma/RuBERT_treetagger_lemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QU5QDetVZB_g"
   },
   "outputs": [],
   "source": [
    "# Prepare the RoBERTa model for training\n",
    "## Learning a RoBERTa base model from scratch\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import RobertaTokenizerFast, RobertaForMaskedLM, RobertaConfig\n",
    "\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "# ./drive/MyDrive/models/lemma/RuBERT_treetagger_lemma\n",
    "# ./drive/MyDrive/models/lemma/RuBERT_udpipe_lemma\n",
    "model_input = \"./drive/MyDrive/models/lemma/RuBERT_treetagger_lemma\"\n",
    "save_dir = model_input\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_input, max_len=512)\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0OseBzOX2bx"
   },
   "outputs": [],
   "source": [
    "# Prepare the RoBERTa model for training\n",
    "## Fine-tuning a pre-trained model\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "\n",
    "# DeepPavlov/rubert-base-cased-sentence\n",
    "# ./drive/MyDrive/models/token/RuBERT_deeppavlov\n",
    "\n",
    "# sberbank-ai/sbert_large_nlu_ru\n",
    "# ./drive/MyDrive/models/token/RuBERT_sberbank\n",
    "model_input = \"DeepPavlov/rubert-base-cased-sentence\"\n",
    "save_dir = \"./drive/MyDrive/models/token/RuBERT_deeppavlov\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_input)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_input)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gyOtY8DY0I4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset # for loading in dataset\n",
    "from transformers import DataCollatorForLanguageModeling # for batching\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "# Load the training data\n",
    "paths = [  \n",
    "  # Lemmatized w/ treetagger\n",
    "  str(Path('./drive/MyDrive/Training_Data/CAT_sentences_full_treetagger_lemma_1.txt')),\n",
    "  str(Path('./drive/MyDrive/Training_Data/CAT_sentences_full_treetagger_lemma_2.txt')),\n",
    "  str(Path('./drive/MyDrive/Training_Data/cybercat_sentences_full_treetagger_lemma.txt'))\n",
    "\n",
    "  # Lemmatized w/ UDPipe\n",
    "  #  str(Path('./drive/MyDrive/Training_Data/CAT_sentences_full_lemma_1.txt')),\n",
    "  #  str(Path('./drive/MyDrive/Training_Data/CAT_sentences_full_lemma_2.txt')),\n",
    "  #  str(Path('./drive/MyDrive/Training_Data/cybercat_sentences_full_lemma.txt'))\n",
    "]\n",
    "\n",
    "for e in range(1, epochs+1) {\n",
    "    curEpoch = f'Epoch[{e}/{epochs}] -'\n",
    "    for file in paths {\n",
    "        # At this moment, this class does not allow for loading multiple files at one :/\n",
    "        print(curEpoch, f\"Loading `{file}`\")\n",
    "        dataset = LineByLineTextDataset(\n",
    "            tokenizer=tokenizer,\n",
    "            file_path=file,\n",
    "            block_size=128,\n",
    "        )\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "        )\n",
    "        print(curEpoch, f\"Finished loading `{file}`\")\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "          output_dir=save_dir,\n",
    "          overwrite_output_dir=True,\n",
    "          num_train_epochs=1,\n",
    "          per_gpu_train_batch_size=32, # Restart runtime & modify this if GPU crashes from low memory: 32,16,8,4,1\n",
    "          per_device_train_batch_size=32, # Restart runtime & modify this if GPU crashes from low memory: 32,16,8,4,1\n",
    "          save_steps=10_000,\n",
    "          save_total_limit=2,\n",
    "      )\n",
    "\n",
    "      # Make sure we are using the most recent model. I don't know if this is necessary; I know know if 'model' updates as it trains\n",
    "      if e != 0 {\n",
    "        model = AutoModelForMaskedLM.from_pretrained(save_dir)\n",
    "      }\n",
    "\n",
    "      trainer = Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          data_collator=data_collator,\n",
    "          train_dataset=dataset\n",
    "      )\n",
    "\n",
    "      trainer.train()\n",
    "\n",
    "      print(curEpoch, f\"Saving model at `{save_dir}`\")\n",
    "      trainer.save_model(save_dir)\n",
    "    }\n",
    "}\n",
    "print(\"Finished training!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPrMnlOxLCjzGIfWQEJQByU",
   "include_colab_link": true,
   "name": "train_models",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
