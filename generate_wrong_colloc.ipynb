{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_wrong_colloc.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO1JY6tisa0DsBJi4fq58DA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iyves/ru_col_suggest/blob/master/generate_wrong_colloc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-CLAPmcbeAX"
      },
      "source": [
        "The purpose of this notebook is in the generation of erroneous Russian academic collocations. The setup is as follows:\n",
        "1. From the cybercat database, extract the 100 most common bi-grams and tri-grams that match a specified PoS filter:\n",
        " - V+N\n",
        " - N+N\n",
        " - Adj+N\n",
        " - V+V\n",
        " - V+Inf\t\t\t\n",
        " - V+Prep+N\n",
        " - N+Prep+N\n",
        "2. For each lemma in each match, substitute the headword with synonyms, queried from the [CrossLexica dictionary](https://www.xl.gelbukh.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtWw3-TFpGU0"
      },
      "source": [
        "### SQL statement for bigrams (tokens):\n",
        "```\n",
        "SELECT * FROM\n",
        "(SELECT a.raw_frequency, \n",
        "    uni1.unigram as \"n1\", uni2.unigram as \"n2\",\n",
        "\tuni1.morph as \"n1_morph\",  uni2.morph as \"n2_morph\"\n",
        "FROM \n",
        "    (SELECT bi.wordform_1, bi.wordform_2, bi.raw_frequency\n",
        "    FROM cybercat.2grams as bi\n",
        "\tORDER BY bi.raw_frequency DESC\n",
        "    LIMIT 10000) as a\n",
        "LEFT JOIN cybercat.unigrams uni1 ON uni1.id_unigram = a.wordform_1\n",
        "LEFT JOIN cybercat.unigrams uni2 ON uni2.id_unigram = a.wordform_2) as matches\n",
        "\n",
        "where \n",
        "    (matches.n1_morph LIKE \"%VerbForm%\" AND \n",
        "\t    (matches.n2_morph LIKE \"%VerbForm%\" OR\n",
        "\t     matches.n2_morph LIKE \"%Animacy%\")) OR\n",
        "\t(matches.n2_morph LIKE \"%Animacy%\" AND \n",
        "\t    (matches.n1_morph LIKE \"%Animacy%\" OR\n",
        "\t\t matches.n1_morph LIKE \"%Degree%\"))\n",
        "\tAND matches.n1 NOT IN (\"А\", \"В\", \"Д\", \"Л\", \"И\", \"УК\", \"Н\", \"М\", \"КС\", \"Ф\", \"Оп\")\n",
        "    AND matches.n2 NOT IN (\"А\", \"В\", \"Д\", \"Л\", \"И\", \"УК\", \"Н\", \"М\", \"КС\", \"Ф\", \"Оп\")\n",
        "LIMIT 300\n",
        "```\n",
        "\n",
        "### SQL statement for bigrams (lemmas):\n",
        "```\n",
        "SELECT raw_frequency, pos1, pos2, l1, l2\n",
        "FROM (\n",
        "\tSELECT tokens.raw_frequency, \n",
        "\t\tlemma1.lemma as \"l1\", lemma2.lemma as \"l2\",\n",
        "\t\tp1.pos as \"pos1\", p2.pos as \"pos2\"\n",
        "\tFROM\n",
        "\t\t(SELECT a.raw_frequency, \n",
        "\t\t\tuni1.lemma as \"n1\", uni2.lemma as \"n2\"\n",
        "\t\tFROM \n",
        "\t\t\t(SELECT bi.wordform_1, bi.wordform_2, bi.raw_frequency\n",
        "\t\t\tFROM cybercat.2grams as bi\n",
        "\t\t\tORDER BY bi.raw_frequency DESC\n",
        "\t\t\tLIMIT 10000\n",
        "\t\t\t) as a\n",
        "\t\t\t\n",
        "\t\tLEFT JOIN cybercat.unigrams uni1 ON uni1.id_unigram = a.wordform_1\n",
        "\t\tLEFT JOIN cybercat.unigrams uni2 ON uni2.id_unigram = a.wordform_2\n",
        "\t\t) as tokens\n",
        "\t\t\n",
        "\tLEFT JOIN cybercat.lemmas lemma1 ON tokens.n1 = lemma1.id_lemmas\n",
        "\tLEFT JOIN cybercat.lemmas lemma2 ON tokens.n2 = lemma2.id_lemmas\n",
        "\tLEFT JOIN cybercat.pos p1 ON p1.id_pos = lemma1.id_pos\n",
        "\tLEFT JOIN cybercat.pos p2 ON p2.id_pos = lemma2.id_pos\n",
        ") as matches\n",
        "\n",
        "WHERE (matches.pos1 = 'VERB' AND matches.pos2 IN ('VERB', 'NOUN')) OR\n",
        "\t(matches.pos1 IN ('NOUN', 'ADJ') AND matches.pos2 = 'NOUN')\n",
        "LIMIT 300\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm4AzmUht2HE"
      },
      "source": [
        "## SQL statement for trigrams (tokens):\n",
        "```\n",
        "SELECT * FROM\n",
        "(SELECT b.raw_frequency,\n",
        "    uni1.unigram as \"n1\", uni2.unigram as \"n2\", uni.unigram as \"n3\",\n",
        "\tuni1.morph as \"n1_morph\", uni2.morph as \"n2_morph\", uni.morph as \"n3_morph\"\n",
        "FROM\n",
        "    (SELECT tri.bigram, tri.token, tri.raw_frequency\n",
        "    FROM cybercat.3grams as tri\n",
        "\tORDER BY tri.raw_frequency DESC\n",
        "    LIMIT 20000) as b\n",
        "LEFT JOIN cybercat.2grams bi on bi.id_bigram = b.bigram\n",
        "LEFT JOIN cybercat.unigrams uni on uni.id_unigram = b.token\n",
        "LEFT JOIN cybercat.unigrams uni1 on uni1.id_unigram = bi.wordform_1\n",
        "LEFT JOIN cybercat.unigrams uni2 on uni2.id_unigram = bi.wordform_2) as matches\n",
        "\n",
        "where\n",
        "    (matches.n3_morph LIKE \"%Animacy%\" AND\n",
        "\t (matches.n2_morph = \"_\" AND\n",
        "         (matches.n2 NOT IN ('NUM', '', ',', '…', ':', '(', ')', '\"', '©'))) AND\n",
        "\t\t(matches.n1_morph LIKE \"%VerbForm%\" OR\n",
        "\t\t matches.n1_morph LIKE \"%Animacy%\"))\n",
        "LIMIT 300\n",
        "```\n",
        "\n",
        "### SQL statement for trigrams (lemmas):\n",
        "```\n",
        "SELECT raw_frequency, pos1, pos2, pos3, l1, l2, l3\n",
        "FROM (\n",
        "\tSELECT tokens.raw_frequency,\n",
        "\t\tlemma1.lemma as \"l1\", lemma2.lemma as \"l2\", lemma3.lemma as \"l3\",\n",
        "\t\tp1.pos as \"pos1\", p2.pos as \"pos2\", p3.pos as \"pos3\"\n",
        "\tFROM (\n",
        "\t\tSELECT b.raw_frequency,\n",
        "\t\t\tuni1.lemma as \"n1\", uni2.lemma as \"n2\", uni.lemma as \"n3\"\n",
        "\t\tFROM\n",
        "\t\t\t(SELECT tri.bigram, tri.token, tri.raw_frequency\n",
        "\t\t\tFROM cybercat.3grams as tri\n",
        "\t\t\tORDER BY tri.raw_frequency DESC\n",
        "\t\t\tLIMIT 20000\n",
        "\t\t\t) as b\n",
        "\t\t\t\n",
        "\t\tLEFT JOIN cybercat.2grams bi on bi.id_bigram = b.bigram\n",
        "\t\tLEFT JOIN cybercat.unigrams uni on uni.id_unigram = b.token\n",
        "\t\tLEFT JOIN cybercat.unigrams uni1 on uni1.id_unigram = bi.wordform_1\n",
        "\t\tLEFT JOIN cybercat.unigrams uni2 on uni2.id_unigram = bi.wordform_2\n",
        "\t) as tokens\n",
        "\n",
        "\tLEFT JOIN cybercat.lemmas lemma1 ON tokens.n1 = lemma1.id_lemmas\n",
        "\tLEFT JOIN cybercat.lemmas lemma2 ON tokens.n2 = lemma2.id_lemmas\n",
        "\tLEFT JOIN cybercat.lemmas lemma3 ON tokens.n3 = lemma3.id_lemmas\n",
        "\tLEFT JOIN cybercat.pos p1 ON p1.id_pos = lemma1.id_pos\n",
        "\tLEFT JOIN cybercat.pos p2 ON p2.id_pos = lemma2.id_pos\n",
        "\tLEFT JOIN cybercat.pos p3 ON p3.id_pos = lemma3.id_pos\n",
        ") as matches\n",
        "\n",
        "WHERE \t\t\t\t\n",
        "\tmatches.pos1 IN ('NOUN', 'VERB') AND\n",
        "\tmatches.pos2 = 'ADP' AND matches.pos3 = 'NOUN'\n",
        "LIMIT 300\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAYiNIT1zr8H"
      },
      "source": [
        "### Generate incorrect collocations through the [CrossLexica](https://www.xl.gelbukh.com/) database\n",
        "**Note:** The code below is the same code as `src/generate_wrong_collocations.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnNy9cVS09fz"
      },
      "source": [
        "import codecs\n",
        "import configparser\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "from pathlib import Path\n",
        "from seleniumrequests import Firefox\n",
        "from selenium import webdriver\n",
        "\n",
        "\n",
        "# Set up the configuration\n",
        "path_current_directory = os.path.dirname(__file__)\n",
        "path_config_file = os.path.join(path_current_directory, '../',\n",
        "                                'config.ini')\n",
        "config = configparser.ConfigParser()\n",
        "config.read(path_config_file)\n",
        "data_dir = config['PATHS']['data_dir']\n",
        "\n",
        "options = webdriver.FirefoxOptions()\n",
        "options.add_argument('start-maximized')\n",
        "options.add_argument('--headless')\n",
        "browser = Firefox(options=options, executable_path='C:/Program Files/geckodriver')\n",
        "\n",
        "\n",
        "def get_synonyms(query: str):\n",
        "    response = browser.request('POST', 'https://www.xl.gelbukh.com/', data={\"query\": query})\n",
        "    full_html = response.text\n",
        "\n",
        "    soup = BeautifulSoup(full_html, 'html.parser')\n",
        "\n",
        "    # extract only lis\n",
        "    lis = soup.find_all('li')\n",
        "    synonyms = set()\n",
        "    read = False\n",
        "    for li in lis:\n",
        "        # if there is an li with class noborder, see if it has an h3 with <font ...>Синонимы</font>\n",
        "        li_a = li.find('a')\n",
        "        if li_a and li_a.has_attr('name'):\n",
        "            if li_a['name'] == \"F_SYN\":\n",
        "                read = True\n",
        "                continue\n",
        "            else:\n",
        "                read = False\n",
        "\n",
        "        # Store subsequent lis if they don't begin with a whitespace\n",
        "        if read and li.string and li.find('a'):\n",
        "            synonym = li.find('a').string\n",
        "            if synonym != query:\n",
        "                synonyms.add(synonym)\n",
        "    return list(synonyms)\n",
        "\n",
        "synonyms = {}\n",
        "bigrams = pd.read_csv(str(Path(data_dir, 'bigram_lemma_raw.csv')), encoding='utf8')\n",
        "\n",
        "with codecs.open(str(Path(data_dir, 'wrong_bigrams.txt')), 'w+', encoding='utf-8') as out_file:\n",
        "    out_file.write(\",\".join([\"raw_frequency\", \"pos1\", \"pos2\", \"l1\", \"l2\", \"syn1\", \"syn2\"]))\n",
        "    for index, row in bigrams.iterrows():\n",
        "        if row['l1'] not in synonyms:\n",
        "            synonyms[row['l1']] = get_synonyms(row['l1'])\n",
        "        if row['l2'] not in synonyms:\n",
        "            synonyms[row['l2']] = get_synonyms(row['l2'])\n",
        "\n",
        "        for synonym in synonyms[row['l1']]:\n",
        "            out_file.write(\"\\n\" + \",\".join([str(row['raw_frequency']), row[\"pos1\"], row[\"pos2\"],\n",
        "                                            row[\"l1\"], row[\"l2\"], synonym, row[\"l2\"]]))\n",
        "        for synonym in synonyms[row['l2']]:\n",
        "            out_file.write(\"\\n\" + \",\".join([str(row['raw_frequency']), row[\"pos1\"], row[\"pos2\"],\n",
        "                                            row[\"l1\"], row[\"l2\"], row['l1'], synonym]))\n",
        "\n",
        "trigrams = pd.read_csv(str(Path(data_dir, 'trigram_lemma_raw.csv')), encoding='utf8')\n",
        "with codecs.open(str(Path(data_dir, 'wrong_trigrams.txt')), 'w+', encoding='utf-8') as out_file:\n",
        "    out_file.write(\",\".join([\"raw_frequency\", \"pos1\", \"pos2\", \"pos3\",\n",
        "                             \"l1\", \"l2\", \"l3\", \"syn1\", \"syn2\", \"syn3\"]))\n",
        "    for index, row in trigrams.iterrows():\n",
        "        if row['l1'] not in synonyms:\n",
        "            synonyms[row['l1']] = get_synonyms(row['l1'])\n",
        "        if row['l3'] not in synonyms:\n",
        "            synonyms[row['l3']] = get_synonyms(row['l3'])\n",
        "\n",
        "        for synonym in synonyms[row['l1']]:\n",
        "            out_file.write(\"\\n\" + \",\".join([str(row['raw_frequency']), row[\"pos1\"], row[\"pos2\"], row[\"pos3\"],\n",
        "                                            row[\"l1\"], row[\"l2\"], row[\"l3\"],\n",
        "                                            synonym, row[\"l2\"], row[\"l3\"]]))\n",
        "        for synonym in synonyms[row['l3']]:\n",
        "            out_file.write(\"\\n\" + \",\".join([str(row['raw_frequency']), row[\"pos1\"], row[\"pos2\"], row[\"pos3\"],\n",
        "                                            row[\"l1\"], row[\"l2\"], row[\"l3\"],\n",
        "                                            row[\"l1\"], row[\"l2\"], synonym]))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}