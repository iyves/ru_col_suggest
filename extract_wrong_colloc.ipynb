{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to extract all erroneous bi- and tri-grams from academic texts. The resulting file is a text file of in-context erroneous n-grams, separeted by new lines.\n",
    "\n",
    "Ex: `В работе я <error> расследовал вопрос </error>`\n",
    "\n",
    "Collocations are selected and saved to a .txt file in the format `pos\\tn-gram\\tcontext\\n`, where `pos` is the part-of-speech (PoS) tag, `colloc` is the extracted collocation, and `context` is the sentence from which the collocation was extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracts all sentences into a hash table of {lemmatized: original}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\whyve\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\treetaggerwrapper.py:740: FutureWarning: Possible nested set at position 8\n",
      "  re.IGNORECASE | re.VERBOSE)\n",
      "c:\\users\\whyve\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\treetaggerwrapper.py:2044: FutureWarning: Possible nested set at position 152\n",
      "  re.VERBOSE | re.IGNORECASE)\n",
      "c:\\users\\whyve\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "c:\\users\\whyve\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from src.html_preprocessor import HtmlPreprocessor\n",
    "from src.tokenizer import Tokenizer\n",
    "\n",
    "file_path = \"./data/evaluation_texts.zip\"\n",
    "\n",
    "tokenizer = Tokenizer(Tokenizer.Method.TREETAGGER)\n",
    "exclude_hyphenated = re.compile(r\"(из|по|ак|что|какого|какой|каких|какая|какое|какие|каким|какому|какими)-\", re.MULTILINE | re.IGNORECASE)\n",
    "hyphenated = re.compile(r\"([А-яЁё])-\", re.MULTILINE)\n",
    "\n",
    "# This sometimes occurs after preprocessing a sentence with quotes, ex: Letters \"a\", \"b\" and \"c\" exist. -> Letters , and exist.\n",
    "skeleton_punct = re.compile(\" [\\\"#$%&'()*+,\\-\\/:;<=>@[\\]^_`{|}~ʹ…〈〉«»—„“]\", re.MULTILINE)\n",
    "space_before_final_period = re.compile(\" \\.$\", re.MULTILINE)\n",
    "\n",
    "# Hashmap, mapping lemmatized example sentences to the associated original sentences\n",
    "examples = {}\n",
    "\n",
    "with ZipFile(file_path, 'r') as zipped:\n",
    "    for file_name in zipped.namelist():\n",
    "        with zipped.open(file_name) as file:\n",
    "            preprocessor = HtmlPreprocessor(\"\", file_name)\n",
    "            preprocessor.phase = HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n",
    "            preprocessor.text = [[]] # Pretend there is only 1 page\n",
    "            \n",
    "            for line in io.TextIOWrapper(file, 'utf-8'):\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Remove empty lines, headers and tables\n",
    "                if line != '' and not line[0].isnumeric() and \\\n",
    "                not line[:8] == \"Таблица \" and not line[:11] == \"Фотография \":\n",
    "                    # De-hyphenate the text\n",
    "                    # Avoid de-hyphenating words like 'из-за' or 'по-русски'\n",
    "                    line = re.sub(exclude_hyphenated, r'\\1--', line)\n",
    "                    line = re.sub(hyphenated, r'\\1', line)\n",
    "                    preprocessor.text[0].append(line)\n",
    "            \n",
    "            preprocessor.substitute_end_of_sentence_punctuation_with_period()\n",
    "            preprocessor.remove_quotations()\n",
    "            preprocessor.remove_empty_paragraphs()\n",
    "            preprocessor.remove_intext_references()\n",
    "            preprocessor.remove_oov_tokens()\n",
    "            preprocessor.replace_numbers()\n",
    "        \n",
    "            preprocessor.break_paragraphs_into_sentences()\n",
    "            preprocessor.remove_unwanted_sentences()\n",
    "            \n",
    "            # Remove title and journal\n",
    "            preprocessor.text[0] = preprocessor.text[0][2:]\n",
    "            \n",
    "            # Lemmatize with treetagger\n",
    "            for i in range(len(preprocessor.text[0])):\n",
    "                for j in range(len(preprocessor.text[0][i])):\n",
    "                    # Remove skeleton lists\n",
    "                    preprocessor.text[0][i][j] = re.sub(skeleton_punct, \"\", preprocessor.text[0][i][j])\n",
    "                \n",
    "                    # Fix instances where there is a space before the final period\n",
    "                    preprocessor.text[0][i][j] = re.sub(space_before_final_period, \".\", preprocessor.text[0][i][j])\n",
    "                \n",
    "                \n",
    "                for original, lemmatized in zip(preprocessor.text[0][i], tokenizer.tokenize(preprocessor.text[0][i])):\n",
    "                    # Remove skeleton lists\n",
    "                    examples[lemmatized.lower()] = original\n",
    "data = [sent.split(\" \") for sent in examples.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify relevant bi- and tri-grams\n",
    "\n",
    "There are two methods for selecting collocations for extraction:\n",
    "    1. Gensim's Phrases library\n",
    "    2. Part-of-Speech filterning\n",
    "\n",
    "The former involves the usage of gensim's Phrases library. Connector words are extracted and passed through this library so as to not impact the calculation of what is determined to be a 'phrase'. These connector words are conjunctions, particles, prepositions, and adverbs.\n",
    "\n",
    "The latter extracts all n-grams that match a PoS template. The `min_freq` and `max_freq` variables are the lower and upper bounds for the acceptible frequency. The following PoS templates are used:\n",
    "\n",
    "- v_n: verb + noun\n",
    "- n_n: noun + noun\n",
    "- a_n: adjective + noun\n",
    "- v_v: verb,inf + verb\n",
    "- v_s_n: verb + preposition + noun\n",
    "- n_s_n: noun + preposition + noun\n",
    "\n",
    "In both methods, only those selected collocations which are not attested in the cybercat database are written to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables\n",
    "save_location = \"./data/extract_wrong_colloc_pos_filter_min2_max3_all.txt\"\n",
    "domain = \"sys\" # or cybercat\n",
    "host = \"localhost\" # or IP address\n",
    "user = \"root\"\n",
    "pwd = \"enter_password_here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'что-то_p', 'куда_p', 'возле_s', 'там_p', 'хотя_q', 'к_s', 'так_p', 'фон_q', 'касательно_s', 'и_c', 'где_p', 'то_p', 'никогда_p', 'кто_p', 'же_c', 'спустя_s', 'откуда_p', 'всегда_p', 'это_p', 'оный_p', 'пока_c', 'под_s', 'что-нибудь_p', 'который_p', 'всетаки_q', 'прежде_s', 'без_s', 'многие_p', 'num_arab-го_q', 'со_s', 'над_s', 'ли_q', 'против_s', 'её_s', 'они_p', 'средь_s', 'об_s', 'оттуда_p', 'каковой_p', 'как-то_p', 'путем_s', 'туда_p', 'самый_p', 'сейчас_p', 'экс_q', 'аль_q', 'ни_q', 'до_s', 'будто_q', 'якобы_q', 'кроме_s', 'де_q', 'вместо_s', 'ваш_p', 'да_c', 'однако_c', 'вроде_s', 'по-другому_p', 'почему_p', 'поскольку_c', 'только_q', 'пусть_c', 'здесь_p', 'его_p', 'перед_s', 'я_p', 'через_s', 'о_s', 'много_p', 'просто_q', 'другой_p', 'то_q', 'при_s', 'нигде_p', 'б_q', 'всякий_p', 'посредством_s', 'либо_c', 'посреди_s', 'напротив_s', 'включая_s', 'сюда_p', 'но_c', 'ввиду_s', 'то_c', 'внутри_s', 'по_s', 'из-за_s', 'у_s', 'никто_p', 'один_p', 'тогда_p', 'иной_p', 'не_q', 'как_p', 'потому_p', 'изо_s', 'сколько_p', 'зачем_p', 'везде_p', 'уж_q', 'по-всякому_p', 'нежели_c', 'как_q', 'выше_s', 'прямо_q', 'наша_p', 'зато_c', 'отчего_p', 'каждый_p', 'суффиксн_q', 'какой_p', 'соответственно_s', 'теперь_p', 'от_s', 'тот_p', 'каков_p', 'именно_q', 'сам_p', 'данный_p', 'остальной_p', 'какой-то_p', 'какой-либо_p', 'сей_p', 'любой_p', 'прочий_p', 'около_s', 'ты_p', 'все_p', 'раз_c', 'подобно_s', 'посредине_s', 'чтобы_c', 'что_c', 'десь_p', 'ведь_q', 'пожалуйста_q', 'тем_c', 'на_s', 'нет_q', 'за_s', 'это_q', 'такой_p', 'поэтому_p', 'иначе_p', 'хотя_c', 'вы_p', 'если_c', 'этот_p', 'столько_p', 'про_s', 'хоть_c', 'в_s', 'потом_p', 'какой-нибудь_p', 'бы_q', 'многое_p', 'их_p', 'же_q', 'весь_p', 'по-своему_p', 'из-под_s', 'ладно_q', 'тоже_q', 'из_s', 'благодаря_s', 'вследствие_s', 'вне_s', 'она_p', 'что_p', 'вот_q', 'всё_s', 'ну_q', 'некоторый_p', 'некий_p', 'или_c', 'там_q', 'тобы_q', 'как_c', 'давай_q', 'согласно_s', 'пусть_q', 'когда_c', 'прочее_p', 'тут_p', 'а_c', 'ничто_p', 'сквозь_s', 'вокруг_s', 'с_s', 'наш_p', 'столь_p', 'он_p', 'всего_q', 'во_s', 'помимо_s', 'между_s', 'другое_p', 'да_q', 'исключая_s', 'среди_s', 'для_s', 'мой_p', 'свое_p', 'лишь_q', 'свой_p', 'также_q', 'ли_c', 'оно_p', 'ко_s', 'таков_p', 'давайте_q', 'чей_p', 'мы_p', 'после_s', 'даже_c', 'даже_q', 'кой_p', 'себя_p', 'многий_p', 'ещё_s', 'вон_q', 'никакой_p', 'хоть_q', 'когда_p', 'чем_c', 'друг_p', 'таковой_p', 'нечто_p', 'ее_p', 'точно_c', 'ни_c', 'спасибо_q'}\n"
     ]
    }
   ],
   "source": [
    "# Get the connector words - for gensim\n",
    "RUSSIAN_CONNECTOR_WORDS = set()\n",
    "for sent in examples.keys():\n",
    "    for word in sent.split(\" \"):\n",
    "        if word[-2:] in (\"_c\", \"_s\", \"_p\", \"_q\"):\n",
    "            RUSSIAN_CONNECTOR_WORDS.add(word)\n",
    "print(RUSSIAN_CONNECTOR_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gensim's phrases library\n",
    "from gensim.models import Phrases\n",
    "\n",
    "bigrams = Phrases(data, \n",
    "                  #min_count=1, threshold=5, \n",
    "                  delimiter=' ', connector_words=RUSSIAN_CONNECTOR_WORDS)\n",
    "trigrams = Phrases(bigrams[data], \n",
    "                   #min_count=1, \n",
    "                   delimiter=' ', connector_words=RUSSIAN_CONNECTOR_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "\n",
    "from src.collocation_attestor import CollocationAttestor\n",
    "\n",
    "with CollocationAttestor(domain=domain, host=host, user=user, password=pwd) as attestor:\n",
    "    with codecs.open(save_location, 'a', 'utf-8') as out_file:\n",
    "        out_file.write(\"pos\\tn-gram\\tcontext\\n\")\n",
    "        for sent in data:\n",
    "            bigrams_ = [b for b in bigrams[sent] if b.count(' ') == 1]\n",
    "            trigrams_ = [t for t in trigrams[bigrams[sent]] if t.count(' ') == 2]\n",
    "\n",
    "            if len(bigrams_) > 0:\n",
    "                for bigram in bigrams_:\n",
    "                    # Check that bigram is not attested\n",
    "                    if \"num_arab\" not in bigram and \\\n",
    "                    len(attestor.attest_collocations([[bi.split(\"_\")[0]] for bi in bigram.split()])) == 0:\n",
    "                        idx = bigrams[sent].index(bigram)\n",
    "                        # Adjust the index if there are bigrams up to idx\n",
    "                        for bi in bigrams[sent][:idx]:\n",
    "                            idx += len(bi.split()) - 1\n",
    "                        \n",
    "                        # Build the in-context erroneous n-gram sentence\n",
    "                        colloc = \" \".join([bi.split(\"_\")[0] for bi in bigram.split()])\n",
    "                        pos = \" \".join([bi.split(\"_\")[1] for bi in bigram.split()])\n",
    "                        \n",
    "                        original = examples[\" \".join(sent)].split()\n",
    "                        context = \" \".join(original[:idx]) + \" <error> \" + \\\n",
    "                              \" \".join(original[idx:idx+2]) + \" </error> \" + \\\n",
    "                              \" \".join(original[idx+2:])\n",
    "                        out_file.write(pos + \"\\t\" + colloc + \"\\t\" + context + \"\\n\")\n",
    "\n",
    "            if len(trigrams_) > 0:\n",
    "                for trigram in trigrams_:\n",
    "                    # Check that trigram is not attested\n",
    "                    if \"num_arab\" not in trigram and \\\n",
    "                    len(attestor.attest_collocations([[tri.split(\"_\")[0]] for tri in trigram.split()])) == 0:\n",
    "                        idx = trigrams[bigrams[sent]].index(trigram)\n",
    "                        # Adjust the index if there are bigrams or trigrams up to idx\n",
    "                        for tri in trigrams[bigrams[sent]][:idx]:\n",
    "                            idx += len(tri.split()) - 1\n",
    "\n",
    "                        # Build the in-context erroneous n-gram sentence\n",
    "                        colloc = \" \".join([tri.split(\"_\")[0] for tri in trigram.split()])\n",
    "                        pos = \" \".join([tri.split(\"_\")[1] for tri in trigram.split()])\n",
    "                        \n",
    "                        original = examples[\" \".join(sent)].split()\n",
    "                        context = \" \".join(original[:idx]) + \" <error> \" + \\\n",
    "                              \" \".join(original[idx:idx+3]) + \" </error> \" + \\\n",
    "                              \" \".join(original[idx+3:])\n",
    "                                                \n",
    "                        out_file.write(pos + \"\\t\" + colloc + \"\\t\" + context + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoS filter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import math\n",
    "\n",
    "from src.collocation_attestor import CollocationAttestor\n",
    "\n",
    "# Use PoS filter\n",
    "filters = [\n",
    "    \"v_n\",\n",
    "    \"n_n\",\n",
    "    \"a_n\",\n",
    "    \"v_v\",\n",
    "    \"v_s_n\", #V+Prep+N\n",
    "    \"n_s_n\" #N+Prep+N\n",
    "]\n",
    "\n",
    "min_freq = 2\n",
    "max_freq = 3\n",
    "\n",
    "name = re.compile(\"[А-яЁё]\\.\")\n",
    "\n",
    "collocations = {}\n",
    "\n",
    "with CollocationAttestor(domain=domain, host=host, user=user, password=pwd) as attestor:\n",
    "    for sent in data:\n",
    "        lemmas, pos_tags = zip(*[word.split(\"_\") for word in sent])\n",
    "        pos_tags = \"_\"+\"_\".join(pos_tags)\n",
    "        original = examples[\" \".join(sent)].split()\n",
    "\n",
    "        # Find all matches for each filter\n",
    "        for f in filters:\n",
    "            for match in re.finditer('(?={0})'.format(re.escape(f)), pos_tags):\n",
    "                start = math.floor(match.start() / 2)\n",
    "                length = math.floor(len(f)/2) + 1\n",
    "\n",
    "                lemma = lemmas[start:start+length]\n",
    "                lemma_str = \" \".join(lemma)\n",
    "                pos_lemma = f + \"&\" + lemma_str\n",
    "\n",
    "                # Check that lemma isn't attested\n",
    "                if \"num_arab\" not in lemma_str and len(lemma_str.strip()) > 0 and \\\n",
    "                len(attestor.attest_collocations([[lem] for lem in lemma])) == 0:\n",
    "                    # Adjust idx if there are names\n",
    "                    for word in original[:start]:\n",
    "                        if name.match(word):\n",
    "                            start += 1\n",
    "\n",
    "                    # Check that there are no names in the ngram\n",
    "                    is_clear = 1\n",
    "                    for word in original[start:start+length]:\n",
    "                        if name.match(word):\n",
    "                            is_clear = 0\n",
    "                            break\n",
    "                    if not is_clear:\n",
    "                        break\n",
    "                        \n",
    "                    # Add the example to the collcations dicitonary\n",
    "                    if pos_lemma not in collocations:\n",
    "                        collocations[pos_lemma] = []\n",
    "                    elif len(collocations[pos_lemma]) > max_freq:\n",
    "                        continue\n",
    "                    \n",
    "                    # Build the in-context erroneous n-gram sentence\n",
    "                    context = \" \".join(original[:start]) + \" <error> \" + \\\n",
    "                        \" \".join(original[start:start+length]) + \" </error> \" + \\\n",
    "                        \" \".join(original[start+length:])\n",
    "                    collocations[pos_lemma].append(context)\n",
    "\n",
    "# Write all the collocations that surpass the min_freq threshold\n",
    "with codecs.open(save_location, 'a', 'utf-8') as out_file:\n",
    "    out_file.write(\"pos\\tn-gram\\tcontext\\n\")\n",
    "    for pos_lemma, contexts in collocations.items():\n",
    "        if len(contexts) < min_freq or len(contexts) > max_freq:\n",
    "            continue\n",
    "        else:\n",
    "            for context in contexts:\n",
    "                pos, lemma = pos_lemma.split(\"&\")\n",
    "                out_file.write(pos + \"\\t\" + lemma + \"\\t\" + context + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
